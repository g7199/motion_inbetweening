import torch
import os
import argparse
from tqdm import tqdm
import numpy as np
import yaml
from types import SimpleNamespace
import math

from diffusion.diffusion import Diffusion
from models.model import MotionTransformer
from data_loaders.data_loader import MotionClipDataset
from bvh_tools.reverse import tensor_to_kinematics
from utils.encode import encode
from utils.inference_tools import frames_to_bvh


def sample_motion_inference(model, scheduler, mean, std, mean_vel, std_vel, device, output_path, output_bvh_path, template_path, 
                          condition_pos, clip_length=180, feature_dim=212, guidance_scale=3.0, is_mp4=True):
    """ì¶”ë¡ ìš© ëª¨ì…˜ ìƒ˜í”Œë§ í•¨ìˆ˜"""
    model.eval()
    
    # ëœë¤ ë…¸ì´ì¦ˆë¡œ ì‹œì‘
    sample = torch.randn((1, clip_length, feature_dim), device=device)
    condition_pos = condition_pos.to(device)
    
    print(f"Starting denoising process with guidance_scale={guidance_scale}")
    
    with torch.no_grad():
        for t in tqdm(range(scheduler.num_timesteps - 1, -1, -1), desc="Sampling"):
            t_tensor = torch.tensor([t], device=device)
            predicted_noise = model.cfg_forward(sample, condition_pos, t_tensor, guidance_scale=guidance_scale)
            sample = scheduler.p_step(predicted_noise, t_tensor, sample)
    
    # ìƒì„±ëœ ëª¨ì…˜ì„ í›„ì²˜ë¦¬
    generated_clip = sample.squeeze(0).cpu().numpy()
    
    # ì •ê·œí™” í•´ì œ
    denormalized_clip = generated_clip * std + mean
    denormalized_clip = denormalized_clip[:, :142]  # contact ì •ë³´ ì œê±°
    
    # BVHë¡œ ë³€í™˜
    root, all_frames_data = tensor_to_kinematics(denormalized_clip, template_path=template_path)
    print(f"Generated motion with {len(all_frames_data)} frames.")
    
    # ì¡°ê±´ ì²˜ë¦¬
    if isinstance(condition_pos, torch.Tensor):
        trajectory_cloned = condition_pos.clone().detach().cpu().numpy()
    else:
        trajectory_cloned = np.array(condition_pos)
    
    if trajectory_cloned.shape[0] == 1:
        trajectory_cloned = trajectory_cloned[0]
    
    # ë¹„ë””ì˜¤ ìƒì„±
    if is_mp4:
        encode(root, all_frames_data, output_filename=output_path, trajectory=trajectory_cloned, traj_mean=mean_vel, traj_std=std_vel)
        print(f"Motion saved to: {output_path}")

    frames_to_bvh(root, all_frames_data, output_bvh_path, template_bvh=template_path)


def load_config(config_path):
    """YAML ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # dictë¥¼ SimpleNamespaceë¡œ ë³€í™˜ (argsì²˜ëŸ¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´)
    def dict_to_namespace(d):
        if isinstance(d, dict):
            for key, value in d.items():
                d[key] = dict_to_namespace(value)
            return SimpleNamespace(**d)
        return d
    
    return dict_to_namespace(config)


def load_model_and_dataset(config):
    """ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ ë¡œë“œ"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")
    
    # ë°ì´í„°ì…‹ ë¡œë“œ (ì •ê·œí™” íŒŒë¼ë¯¸í„°ë¥¼ ìœ„í•´)
    dataset = MotionClipDataset(
        config.bvh_dir, 
        clip_length=config.clip_length, 
        feat_bias=config.feat_bias, 
        height_threshold=config.height_threshold, 
        velocity_threshold=config.velocity_threshold
    )
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    model = MotionTransformer(
        feature_dim=config.feature_dim,
        latent_dim=config.latent_dim,
        num_layers=config.num_layers,
        ff_size=config.ff_size,
        nhead=config.nhead,
        dropout=config.dropout,
        activation=config.activation,
        uncond_prob=config.uncond_prob
    ).to(device)
    
    # ë””í“¨ì „ ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”
    diffusion = Diffusion(num_timesteps=1000, device=device)
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    if not os.path.isfile(config.checkpoint):
        raise FileNotFoundError(f"Checkpoint not found: {config.checkpoint}")
    
    print(f"Loading checkpoint: {config.checkpoint}")
    checkpoint = torch.load(config.checkpoint, map_location=device)
    
    # ì²´í¬í¬ì¸íŠ¸ í˜•ì‹ í™•ì¸ í›„ ë¡œë“œ
    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        epoch = checkpoint.get('epoch', 'unknown')
        print(f"Loaded checkpoint from epoch {epoch}")
    else:
        # ê¸°ì¡´ í˜•ì‹: ë‹¨ìˆœ state_dict
        model.load_state_dict(checkpoint)
        print("Loaded legacy checkpoint (model state only)")
    
    return model, diffusion, dataset, device


def generate_random_trajectory(clip_length, trajectory_type="straight"):
    """ëœë¤ ê¶¤ì ì„ ì†ë„ë¡œ ìƒì„± (yaw_vel í¬í•¨)"""
    if trajectory_type == "straight":
        # ì§ì„  ê¶¤ì 
        start_x, start_z = 0.0, 0.0
        end_x, end_z = np.random.uniform(-5, 5), np.random.uniform(5, 10)
        x_coords = np.linspace(start_x, end_x, clip_length)
        z_coords = np.linspace(start_z, end_z, clip_length)
        
        # ì§ì„ ì´ë¯€ë¡œ yawëŠ” ì¼ì •í•œ ë°©í–¥
        target_yaw = math.atan2(end_x - start_x, end_z - start_z)  # ëª©í‘œ ë°©í–¥
        yaw_coords = np.full(clip_length, target_yaw)  # ì¼ì •í•œ ê°ë„
    
    elif trajectory_type == "circle":
        # ì›í˜• ê¶¤ì 
        radius = np.random.uniform(2, 5)
        center_x, center_z = 0.0, radius
        angles = np.linspace(0, 2*np.pi, clip_length)
        x_coords = center_x + radius * np.cos(angles)
        z_coords = center_z + radius * np.sin(angles)
        
        # ì›í˜• ê¶¤ì ì˜ yaw: í•­ìƒ ì ‘ì„  ë°©í–¥
        yaw_coords = angles + np.pi/2  # ì ‘ì„  ë°©í–¥ (90ë„ íšŒì „)
    
    elif trajectory_type == "zigzag":
        # ì§€ê·¸ì¬ê·¸ ê¶¤ì 
        amplitude = np.random.uniform(2, 4)
        frequency = np.random.uniform(0.02, 0.05)
        z_coords = np.linspace(0, 10, clip_length)
        x_coords = amplitude * np.sin(frequency * 2 * np.pi * np.arange(clip_length))
        
        # ì§€ê·¸ì¬ê·¸ì˜ yaw: ì›€ì§ì„ ë°©í–¥ì— ë”°ë¼ ê³„ì‚°
        dx = np.gradient(x_coords)  # x ë°©í–¥ ê¸°ìš¸ê¸°
        dz = np.gradient(z_coords)  # z ë°©í–¥ ê¸°ìš¸ê¸°
        yaw_coords = np.arctan2(dx, dz)  # ì›€ì§ì„ ë°©í–¥
    
    else:
        raise ValueError(f"Unknown trajectory type: {trajectory_type}")
    
    # ìœ„ì¹˜ë¥¼ ì†ë„ë¡œ ë³€í™˜
    positions = np.column_stack([x_coords, z_coords])  # [T, 2]
    
    # XZ ì†ë„ ê³„ì‚°
    velocities_xz = np.zeros_like(positions)
    velocities_xz[:-1] = positions[1:] - positions[:-1]
    velocities_xz[-1] = velocities_xz[-2]  # ë§ˆì§€ë§‰ í”„ë ˆì„
    
    # Yaw ì†ë„ ê³„ì‚°
    yaw_velocities = np.zeros(clip_length)
    yaw_velocities[:-1] = yaw_coords[1:] - yaw_coords[:-1]
    
    # Yaw ì†ë„ì—ì„œ ê°ë„ wrap-around ì²˜ë¦¬
    yaw_velocities = np.where(yaw_velocities > np.pi, yaw_velocities - 2*np.pi, yaw_velocities)
    yaw_velocities = np.where(yaw_velocities < -np.pi, yaw_velocities + 2*np.pi, yaw_velocities)
    
    yaw_velocities[-1] = yaw_velocities[-2]  # ë§ˆì§€ë§‰ í”„ë ˆì„
    
    # [T, 3] í˜•íƒœë¡œ ê²°í•©: [vel_x, vel_z, yaw_vel]
    velocities_3d = np.column_stack([velocities_xz[:, 0], velocities_xz[:, 1], yaw_velocities])
    
    return torch.FloatTensor(velocities_3d*100).unsqueeze(0) 


def inference(config):
    """ì¶”ë¡  ë©”ì¸ í•¨ìˆ˜"""
    # ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ ë¡œë“œ
    model, diffusion, dataset, device = load_model_and_dataset(config)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± (configì˜ inference_dir ì‚¬ìš©)
    os.makedirs(config.inference_dir, exist_ok=True)
    if config.is_mp4:
        os.makedirs(config.inference_dir+"/mp4", exist_ok=True)
    os.makedirs(config.inference_dir+"/bvh", exist_ok=True)
    
    for i in range(config.num_samples):
        print(f"\n--- Generating sample {i+1}/{config.num_samples} ---")
        
        # ì¡°ê±´ ìƒì„±
        if hasattr(config, 'condition_file') and config.condition_file:
            print(f"Loading condition from: {config.condition_file}")
            condition_pos = torch.load(config.condition_file, map_location='cpu')

            # ë¦¬ìŠ¤íŠ¸ë©´ í•˜ë‚˜ ë½‘ê¸°
            if isinstance(condition_pos, list):
                condition_pos = condition_pos[i % len(condition_pos)]

            # í…ì„œ ê°•ì œ ìºìŠ¤íŒ…
            if isinstance(condition_pos, np.ndarray):
                condition_pos = torch.from_numpy(condition_pos)

            # (T,3) -> (1,T,3)
            if condition_pos.dim() == 2:
                condition_pos = (condition_pos - dataset.mean_vel) / dataset.std_vel
                print(condition_pos)
                condition_pos = condition_pos.unsqueeze(0)

            # dtype, device ì •ë¦¬
            condition_pos = condition_pos.to(dtype=torch.float32)
        else:
            # ëœë¤ ê¶¤ì  ìƒì„±
            trajectory_types = ["straight", "circle", "zigzag"]
            trajectory_type = np.random.choice(trajectory_types)
            print(f"Generating random {trajectory_type} trajectory")
            condition_pos = generate_random_trajectory(config.clip_length, trajectory_type)
        
        # ìƒ˜í”Œ ìƒì„±
        output_path = os.path.join(config.inference_dir, f"mp4/sample_{i+1:03d}.mp4")
        output_bvh_path = os.path.join(config.inference_dir, f"bvh/sample_{i+1:03d}.bvh")
        
        try:
            sample_motion_inference(
                model=model,
                scheduler=diffusion,
                mean=dataset.mean,
                std=dataset.std,
                mean_vel=dataset.mean_vel,
                std_vel=dataset.std_vel,
                device=device,
                output_path=output_path,
                output_bvh_path=output_bvh_path,
                template_path=config.template_bvh,
                condition_pos=condition_pos,
                clip_length=config.clip_length,
                feature_dim=config.feature_dim,
                guidance_scale=config.guidance_scale,
                is_mp4=config.is_mp4
            )
            print(f"âœ“ Sample {i+1} generated successfully")
        
        except Exception as e:
            print(f"âœ— Error generating sample {i+1}: {e}")
            continue
    
    print(f"\nğŸ‰ Inference completed! Generated samples saved in: {config.inference_dir}")


def main():
    parser = argparse.ArgumentParser(description="Motion Diffusion Inference")
    
    # í•„ìˆ˜ íŒŒë¼ë¯¸í„°ë§Œ
    parser.add_argument('--config', type=str, required=True,
                        help='Path to config YAML file')
    parser.add_argument('--checkpoint', type=str, required=True,
                        help='Path to model checkpoint')
    parser.add_argument('--num_samples', type=int, default=5,
                        help='Number of samples to generate')
    parser.add_argument('--condition_file', type=str, default=None,
                        help='Path to condition file (.pt). If not provided, random trajectories will be generated')
    parser.add_argument('--mp4', type=str, default=False,
                        help='making mp4')
    
    args = parser.parse_args()
    
    # config íŒŒì¼ ë¡œë“œ
    config = load_config(args.config)
    
    # ëª…ë ¹ì¤„ ì¸ìë¡œ í•„ìš”í•œ ê²ƒë§Œ ë®ì–´ì“°ê¸°
    config.checkpoint = args.checkpoint
    config.num_samples = args.num_samples
    config.is_mp4 = args.mp4
    if args.condition_file:
        config.condition_file = args.condition_file
    
    # íŒŒë¼ë¯¸í„° ì¶œë ¥
    print("=== Inference Configuration ===")
    print(f"Config file: {args.config}")
    print(f"Checkpoint: {config.checkpoint}")
    print(f"Output dir: {config.inference_dir}")
    print(f"Num samples: {config.num_samples}")
    print(f"Guidance scale: {config.guidance_scale}")
    if hasattr(config, 'condition_file'):
        print(f"Condition file: {config.condition_file}")
    print("=" * 30)
    
    inference(config)


if __name__ == "__main__":
    main()