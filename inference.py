import torch
import os
import argparse
from tqdm import tqdm
import numpy as np
import yaml
from types import SimpleNamespace
import glob
from pyglm import glm

# BVH ì²˜ë¦¬ ê´€ë ¨
from bvh_tools.bvh_controller import parse_bvh, get_preorder_joint_list
from data_loaders.data_sampler import get_data

# ëª¨ë¸ ë° ì¶”ë¡  ê´€ë ¨
from diffusion.diffusion import Diffusion
from models.model import MotionTransformer
from data_loaders.data_loader import MotionClipDataset
from bvh_tools.reverse import tensor_to_kinematics
from utils.encode import encode
from utils.inference_tools import frames_to_bvh, get_joint_positions_dfs
from utils.keyframe import KeyframeSelector

selector = KeyframeSelector(ratio=0.1, mode='uniform')

def extract_all_frames_positions(motion, virtual_root, max_frames=None, is_original=False, start_frame=0):
    """ì´ë¯¸ ê³„ì‚°ëœ motionê³¼ virtual_rootë¡œ ëª¨ë“  í”„ë ˆì„ì˜ joint positions ì¶”ì¶œ"""
    
    # ì „ì²´ í”„ë ˆì„ ìˆ˜ í™•ì¸
    total_frames = motion.frames
    if max_frames is not None:
        total_frames = min(total_frames, max_frames)
    
    print(f"Total frames to process: {total_frames}")
    
    all_frames_positions = []
    first_kin = None  # is_original=Trueì¼ ë•Œ ì²« ë²ˆì§¸ í”„ë ˆì„ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€í™”
    
    for frame_idx in tqdm(range(total_frames), desc="Extracting positions"):

        # í•´ë‹¹ í”„ë ˆì„ ì„¤ì •
        if is_original:
            frame_idx += start_frame
        motion.apply_to_skeleton(frame_idx, virtual_root)
        
        if is_original:
            # ì²« ë²ˆì§¸ ì½”ë“œì™€ ë™ì¼í•œ ìƒëŒ€í™” ê³¼ì • ì ìš©
            global_kinematics = np.array(virtual_root.kinematics)
            
            if first_kin is None:
                # ì²« ë²ˆì§¸ í”„ë ˆì„ì„ ê¸°ì¤€ìœ¼ë¡œ ì €ì¥
                first_kin = global_kinematics.copy()
                virtual_root.kinematics = glm.mat4(1.0)
            else:
                # ì²« ë²ˆì§¸ í”„ë ˆì„ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€í™”
                first_inv = np.linalg.inv(first_kin)
                relative_kinematics = first_inv @ global_kinematics
                
                # GLMìœ¼ë¡œ ì ìš©
                virtual_root.kinematics = glm.mat4(*relative_kinematics.T.flatten())
        
        # position ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        position_list = []
        
        # DFSë¡œ position ì¶”ì¶œ
        get_joint_positions_dfs(virtual_root, position_list)
        
        all_frames_positions.append(np.array(position_list[3:]))
    
    return np.array(all_frames_positions)  # shape: [frames, joints, 3]

def compare_bvh_positions(original_bvh, generated_bvh, max_frames=180, start_frame=0):
    """ì›ë³¸ BVHì™€ ìƒì„±ëœ BVHì˜ joint positions ë¹„êµ"""
    print("\n=== BVH Position Comparison ===")
    
    # ì›ë³¸ BVH positions ì¶”ì¶œ
    print("Extracting original BVH positions...")
    root_orig, motion_orig = parse_bvh(original_bvh)
    joint_order_orig = get_preorder_joint_list(root_orig)
    motion_orig.build_quaternion_frames(joint_order_orig)
    virtual_root_orig = motion_orig.apply_virtual(root_orig)
    original_positions = extract_all_frames_positions(motion_orig, virtual_root_orig, max_frames, is_original=True, start_frame=start_frame)
    print(original_bvh)
    
    
    # ìƒì„±ëœ BVH positions ì¶”ì¶œ
    print("Extracting generated BVH positions...")
    root_gen, motion_gen = parse_bvh(generated_bvh)
    joint_order_gen = get_preorder_joint_list(root_gen)
    motion_gen.build_quaternion_frames(joint_order_gen)
    virtual_root_gen = motion_gen.apply_virtual(root_gen)
    generated_positions = extract_all_frames_positions(motion_gen, virtual_root_gen, max_frames, is_original=False, start_frame=0)
    
    print(f"\nOriginal shape: {original_positions.shape}")
    print(f"Generated shape: {generated_positions.shape}")
    
    # í˜•íƒœê°€ ë‹¤ë¥´ë©´ ê²½ê³ 
    if original_positions.shape != generated_positions.shape:
        print("WARNING: Position array shapes don't match!")
        min_frames = min(original_positions.shape[0], generated_positions.shape[0])
        min_joints = min(original_positions.shape[1], generated_positions.shape[1])
        print(f"Using common dimensions: frames={min_frames}, joints={min_joints}")
        original_positions = original_positions[:min_frames, :min_joints]
        generated_positions = generated_positions[:min_frames, :min_joints]
    
    # ì°¨ì´ ê³„ì‚°
    diff = original_positions - generated_positions  # signed ì°¨ì´ (ë¨¼ì € ê³„ì‚°)
    position_diff = np.abs(diff)  # ì ˆëŒ€ ì°¨ì´ (MAE ê¸°ë°˜ìš©)
    squared_diff = diff ** 2  # ì œê³± ì°¨ì´ (MSE ê¸°ë°˜ìš©)

    # ì „ì²´ MSEì™€ RMSE ê³„ì‚° (squared_diff ì‚¬ìš©ìœ¼ë¡œ ì¤‘ë³µ ì œê±°)
    mse = np.mean(squared_diff)  # ì „ì²´ MSE
    rmse = np.sqrt(mse)  # í‘œì¤€ RMSE: signed diff ê¸°ë°˜

    # í†µê³„ ì¶œë ¥
    print(f"\nPosition Comparison Statistics:")
    print(f"Mean absolute difference: {np.mean(position_diff):.6f}")
    print(f"Max absolute difference: {np.max(position_diff):.6f}")
    print(f"RMSE: {rmse:.6f}")  # ìˆ˜ì •: í‘œì¤€ RMSEë¡œ
    print(f"MSE: {mse:.6f}")

    # í”„ë ˆì„ë³„ MSE (ê¸°ì¡´ ê·¸ëŒ€ë¡œ)
    frame_mses = np.mean(squared_diff, axis=(1, 2))  # ê° í”„ë ˆì„ì˜ joint(24)ì™€ coord(3) í‰ê·  MSE
    print(f"\nFrame-wise MSEs:")
    print(f"Min frame MSE: {np.min(frame_mses):.6f}")
    print(f"Max frame MSE: {np.max(frame_mses):.6f}")
    print(f"Mean frame MSE: {np.mean(frame_mses):.6f}")  # ì°¸ê³ : ì´ê²Œ ì „ì²´ mseì™€ ê°™ìŒ

    # Jointë³„ MSE (ê¸°ì¡´ ê·¸ëŒ€ë¡œ)
    joint_mses = np.mean(squared_diff, axis=(0, 2))  # ê° ì¡°ì¸íŠ¸ì˜ frame(180)ì™€ coord(3) í‰ê·  MSE
    print(f"\nJoint-wise MSEs:")
    for i, mse in enumerate(joint_mses):
        print(f"Joint {i}: {mse:.6f}")
    
    return original_positions, generated_positions, mse

def process_bvh_and_inference(bvh_file_path, config_path='config.yml', start_frame=600, clip_length=180, add_position=False):
    """BVH íŒŒì¼ì—ì„œ ìƒ˜í”Œ ìƒì„±í•˜ê³  ë°”ë¡œ ì¶”ë¡ ê¹Œì§€ ìˆ˜í–‰"""
    print(f"Processing BVH: {bvh_file_path}")
    
    # config ë¡œë“œ
    with open(config_path, 'r') as f:
        config_dict = yaml.safe_load(f)
    feat_bias = config_dict.get('feat_bias', 1.0)
    posed_ratio = config_dict.get('posed_ratio', 0.1)
    
    def dict_to_namespace(d):
        if isinstance(d, dict):
            for key, value in d.items():
                d[key] = dict_to_namespace(value)
            return SimpleNamespace(**d)
        return d
    
    config = dict_to_namespace(config_dict)
    
    print("Step 1: Creating sample data from BVH...")
    
    # BVH íŒŒì‹± ë° ë°ì´í„° ì¶”ì¶œ (í•œ ë²ˆë§Œ)
    root, motion = parse_bvh(bvh_file_path)
    joint_order = get_preorder_joint_list(root)
    motion.build_quaternion_frames(joint_order)
    virtual_root = motion.apply_virtual(root)
    clip_data, traj = get_data(motion, virtual_root, time_size=clip_length, start_frame=start_frame)
    
    # mean, std ë¡œë“œ (í›ˆë ¨ ë°ì´í„°ì™€ ë™ì¼í•œ ì •ê·œí™” ì ìš©)
    mean = np.load('data/mean_pos.npy')
    std = np.load('data/std_pos.npy')
    std += 1e-8
    std[:4] /= feat_bias
    std[-4:] /= feat_bias
    
    # trajectory ê´€ë ¨ í†µê³„
    mean_vel = mean[:3]
    std_vel = std[:3]
    
    # í…ì„œ ë³€í™˜ ë° ì •ê·œí™”
    clip_data_tensor = torch.tensor(clip_data, dtype=torch.float32).unsqueeze(0)
    traj_tensor = torch.tensor(traj, dtype=torch.float32).unsqueeze(0)
    clip_data_tensor = (clip_data_tensor - torch.from_numpy(mean).float()) / torch.from_numpy(std).float()
    traj_tensor = (traj_tensor - torch.from_numpy(mean_vel).float()) / torch.from_numpy(std_vel).float()
    
    print(f"Clip data shape: {clip_data_tensor.shape}, dtype: {clip_data_tensor.dtype}")
    print(f"Trajectory shape: {traj_tensor.shape}, dtype: {traj_tensor.dtype}")
    
    # posed_indices ìƒì„±
    _, posed_indices = selector.select_keyframes_by_ratio(clip_data_tensor)
    
    print(f"Generated posed_indices: {posed_indices[0]} (total: {len(posed_indices[0])} frames)")
    
    # ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„
    sample_data = {
        'clean_motions': clip_data_tensor,
        'conditions': traj_tensor,
        'posed_indices': posed_indices,
        'dataset_stats': {
            'mean': torch.from_numpy(mean).float(),
            'std': torch.from_numpy(std).float(),
            'mean_vel': torch.from_numpy(mean_vel).float(),
            'std_vel': torch.from_numpy(std_vel).float()
        }
    }
    
    print("\nStep 2: Loading model and starting inference...")
    
    # ëª¨ë¸ ë¡œë“œ
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")
    
    model = MotionTransformer(
        feature_dim=config.feature_dim,
        latent_dim=config.latent_dim,
        num_layers=config.num_layers,
        ff_size=config.ff_size,
        nhead=config.nhead,
        dropout=config.dropout,
        activation=config.activation,
        uncond_prob=config.uncond_prob
    ).to(device)
    
    diffusion = Diffusion(num_timesteps=1000, device=device)
    
    if not os.path.isfile(config.checkpoint):
        raise FileNotFoundError(f"Checkpoint not found: {config.checkpoint}")
    
    print(f"Loading checkpoint: {config.checkpoint}")
    checkpoint = torch.load(config.checkpoint, map_location=device)
    
    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        epoch = checkpoint.get('epoch', 'unknown')
        print(f"Loaded checkpoint from epoch {epoch}")
    else:
        model.load_state_dict(checkpoint)
        print("Loaded legacy checkpoint (model state only)")
    
    # ìƒ˜í”Œ ë°ì´í„°ì—ì„œ ì •ë³´ ì¶”ì¶œ
    feature_dim = sample_data['clean_motions'].shape[-1]
    condition_pos = sample_data['conditions'][0:1]
    clean_motion = sample_data['clean_motions'][0:1]
    posed_indices = sample_data['posed_indices'][0:1]
    posed_data = clean_motion[:, posed_indices[0]]
    
    print(f"Using clip_length: {clip_length}, feature_dim: {feature_dim}")
    print(f"Sample info:")
    print(f"- Condition shape: {condition_pos.shape}")
    print(f"- Posed indices: {posed_indices[0]}")
    print(f"- Posed data shape: {posed_data.shape}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(config.inference_dir, exist_ok=True)
    if config.is_mp4:
        os.makedirs(config.inference_dir+"/mp4", exist_ok=True)
    os.makedirs(config.inference_dir+"/bvh", exist_ok=True)
    
    # ì¶”ë¡  ìˆ˜í–‰
    generated_bvh_paths = []
    for i in range(config.num_samples):
        print(f"\n--- Generating sample {i+1}/{config.num_samples} ---")
        
        output_path = os.path.join(config.inference_dir, f"mp4/sample_{i+1:03d}.mp4")
        output_bvh_path = os.path.join(config.inference_dir, f"bvh/sample_{i+1:03d}.bvh")
        
        try:
            mse = sample_motion_inference(
                model=model,
                scheduler=diffusion,
                mean=sample_data['dataset_stats']['mean'],
                std=sample_data['dataset_stats']['std'],
                mean_vel=sample_data['dataset_stats']['mean_vel'],
                std_vel=sample_data['dataset_stats']['std_vel'],
                device=device,
                output_path=output_path,
                output_bvh_path=output_bvh_path,
                template_path=config.template_bvh,
                condition_pos=condition_pos,
                clip_length=clip_length,
                feature_dim=feature_dim,
                guidance_scale=config.guidance_scale,
                is_mp4=config.is_mp4,
                posed_data=posed_data,
                posed_indices=posed_indices,
                original_bvh_path=bvh_file_path,
                start_frame=start_frame,
                add_position=add_position
            )
            generated_bvh_paths.append(output_bvh_path)
        
        except Exception as e:
            print(f"âœ— Error generating sample {i+1}: {e}")
            continue
    
    print(f"\nğŸ‰ All done! Generated samples saved in: {config.inference_dir}")

def sample_motion_inference(model, scheduler, mean, std, mean_vel, std_vel, device, output_path, output_bvh_path, template_path, 
                         condition_pos, clip_length=180, feature_dim=212, guidance_scale=3.0, is_mp4=True,
                         posed_data=None, posed_indices=None, original_bvh_path=None, start_frame=0, add_position=False):
    """ì¶”ë¡ ìš© ëª¨ì…˜ ìƒ˜í”Œë§ í•¨ìˆ˜"""
    model.eval()
    
    # ëœë¤ ë…¸ì´ì¦ˆë¡œ ì‹œì‘ (float32ë¡œ ëª…ì‹œì  ì§€ì •)
    sample = torch.randn((1, clip_length, feature_dim), device=device, dtype=torch.float32)
    condition_pos = condition_pos.to(device, dtype=torch.float32)
    
    # posed_dataê°€ ìˆìœ¼ë©´ deviceë¡œ ì´ë™í•˜ê³  dtype ë§ì¶”ê¸°
    if posed_data is not None:
        posed_data = posed_data.to(device, dtype=torch.float32)
        
        # posed_indicesê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬
        if isinstance(posed_indices, list):
            # ë¦¬ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸ ìš”ì†Œê°€ í…ì„œì¸ ê²½ìš°
            if len(posed_indices) > 0 and isinstance(posed_indices[0], torch.Tensor):
                posed_indices = posed_indices[0].unsqueeze(0).to(device)
            else:
                # ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜
                posed_indices = torch.tensor(posed_indices, device=device)
        else:
            posed_indices = posed_indices.to(device)
            
        print(f"Using posed constraints at {len(posed_indices[0]) if posed_indices.dim() > 1 else len(posed_indices)} frames")
        print(f"Posed indices: {posed_indices[0] if posed_indices.dim() > 1 else posed_indices}")
        print(f"Posed data shape: {posed_data.shape}")
    
    print(f"Starting denoising process with guidance_scale={guidance_scale}")
    
    with torch.no_grad():
        for t in tqdm(range(scheduler.num_timesteps - 1, -1, -1), desc="Sampling"):
            t_tensor = torch.tensor([t], device=device)
            predicted_noise = model.cfg_forward(sample, condition_pos, posed_data, posed_indices, t_tensor, guidance_scale=guidance_scale)
            sample = scheduler.p_step(predicted_noise, t_tensor, sample)
            
    # ë§¨ ë§ˆì§€ë§‰ì— posed constraint ì ìš©
    if posed_data is not None and posed_indices is not None:
        if sample.dtype != posed_data.dtype:
            posed_data = posed_data.to(sample.dtype)
        sample[:, posed_indices[0]] = posed_data
    
    # ìƒì„±ëœ ëª¨ì…˜ì„ í›„ì²˜ë¦¬
    generated_clip = sample.squeeze(0).cpu().numpy()
    
    # mean, stdë¥¼ numpyë¡œ ë³€í™˜
    if isinstance(mean, torch.Tensor):
        mean = mean.cpu().numpy()
    if isinstance(std, torch.Tensor):
        std = std.cpu().numpy()
    if isinstance(mean_vel, torch.Tensor):
        mean_vel = mean_vel.cpu().numpy()
    if isinstance(std_vel, torch.Tensor):
        std_vel = std_vel.cpu().numpy()
    
    # ì •ê·œí™” í•´ì œ
    denormalized_clip = generated_clip * std + mean
    denormalized_clip = denormalized_clip[:, :142]  # contact ì •ë³´ ì œê±°
    
    # BVHë¡œ ë³€í™˜
    root, all_frames_data = tensor_to_kinematics(denormalized_clip, template_path=template_path)
    print(f"Generated motion with {len(all_frames_data)} frames.")
    
    # ì¡°ê±´ ì²˜ë¦¬
    if isinstance(condition_pos, torch.Tensor):
        trajectory_cloned = condition_pos.clone().detach().cpu().numpy()
    else:
        trajectory_cloned = np.array(condition_pos)
    
    if trajectory_cloned.shape[0] == 1:
        trajectory_cloned = trajectory_cloned[0]
    
    frames_to_bvh(root, all_frames_data, output_bvh_path, template_bvh=template_path)
    
    # compareì—ì„œ positions ê³„ì‚°
    original_positions, generated_positions, mse = compare_bvh_positions(
        original_bvh=original_bvh_path,
        generated_bvh=output_bvh_path,
        max_frames=clip_length,
        start_frame=start_frame
    )

    if not add_position:
        generated_positions = None
        original_positions = None
    
    # ë¹„ë””ì˜¤ ìƒì„± (compareì—ì„œ êµ¬í•œ positions ì‚¬ìš©)
    if is_mp4:
        encode(root, all_frames_data, output_filename=output_path, 
               trajectory=trajectory_cloned, traj_mean=mean_vel, traj_std=std_vel,
               positions=generated_positions, original_positions=original_positions)
        print(f"Motion saved to: {output_path}")

    print(f"MSE: {mse:.6f}")
    return mse

def main():
    parser = argparse.ArgumentParser(description="Motion Diffusion BVH Processing and Inference")
    
    parser.add_argument('--bvh_file', type=str, required=True,
                        help='Path to BVH file')
    parser.add_argument('--start_frame', type=int, default=600,
                        help='Start frame for BVH clipping')
    parser.add_argument('--clip_length', type=int, default=180,
                        help='Length of clip in frames')
    parser.add_argument('--config', type=str, default='config.yml',
                        help='Config file path')
    parser.add_argument('--compare_only', action='store_true',
                        help='Only compare two BVH files without inference')
    parser.add_argument('--generated_bvh', type=str,
                        help='Path to generated BVH file for comparison')
    parser.add_argument('--add_position', type=bool, default=False,
                        help='Add position')

    args = parser.parse_args()
    
    if args.compare_only and args.generated_bvh:
        # ë¹„êµë§Œ ìˆ˜í–‰
        print("=== BVH Comparison Mode ===")
        compare_bvh_positions(args.bvh_file, args.generated_bvh, args.clip_length, start_frame=args.start_frame)
    else:
        # ì „ì²´ í”„ë¡œì„¸ìŠ¤ ìˆ˜í–‰
        print("=== Motion Diffusion BVH Processing ===")
        print(f"BVH file: {args.bvh_file}")
        print(f"Start frame: {args.start_frame}")
        print(f"Clip length: {args.clip_length}")
        print(f"Config file: {args.config}")
        print("=" * 38)
        
        process_bvh_and_inference(
            args.bvh_file,
            args.config,
            args.start_frame,
            args.clip_length,
            args.add_position
        )

if __name__ == "__main__":
    main()